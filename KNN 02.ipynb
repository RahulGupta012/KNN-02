{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a087234f-8c58-4f36-903d-5ef4037d6010",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #C0AFDD; color: black; text-align: center; padding: 10px; border-radius: 10px; \">\n",
    "    <h1>KNN 02</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6737e-1f84-4332-8921-d1d714492f40",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612a1b6-39b7-492b-8672-37fbf2c992e9",
   "metadata": {},
   "source": [
    "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eaa497-f8c9-4718-a5de-b419e1a42563",
   "metadata": {},
   "source": [
    "The main difference between Euclidean distance metric and the Manhattan distance metric in KNN, is technique of measuring the distance between the data points ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b5c1a-ed95-42e8-a16e-4ddffa5a5e3c",
   "metadata": {},
   "source": [
    "| Aspect                | Euclidean Distance                               | Manhattan Distance                               |\n",
    "|-----------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| Measurement Type      | Straight-line distance                           | Sum of absolute differences along axes           |\n",
    "| Path Consideration    | Diagonal paths included                          | Only horizontal and vertical paths                |\n",
    "| Sensitivity to Scales  | Sensitive to feature scales                      | Less sensitive to feature scales                  |\n",
    "| Dimensionality        | Prone to curse of dimensionality                  | Less affected by curse of dimensionality          |\n",
    "| Anisotropy            | Suitable for isotropic relationships             | Can be suitable for anisotropic relationships     |\n",
    "| Common Use Cases      | Isotropic features, similar scales               | Anisotropic features, different scales            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dafe70-bc18-4bf3-9aa1-5d05faaf450f",
   "metadata": {},
   "source": [
    "**Effects on performance of regressor and classifier :**\n",
    "\n",
    "Euclidean distance is sensitive to outliers because it squares the differences. Outliers can have a disproportionately large effect on the overall distance.\n",
    "Manhattan distance is less sensitive to outliers since it only considers the absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092466e-7703-4ed4-bc72-de02a011528c",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240422c-ed46-4670-ac1f-715f2402d00d",
   "metadata": {},
   "source": [
    "To choosing the optimal value of k means , we set the number of neghbiours of the new data point. The choice of k value is play a crucial role to the performance of the algorithem. We can decide the best value of k according to our dataset by certain methods ;\n",
    "\n",
    "  **Experiment with values  :** We can experiment with the values to see how it works in a certain situation. And the value which gives high accuracy ,we can considered as best optimal value of k.\n",
    "  \n",
    "  **Hyperparameter tunning :** It is the best way to identify the best optimal value for k  in the knn algorithem. In this method , it automatically search the best value of given parameter for our case. Gridsearch cv and randamized search cv can used for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d9f86-8bd1-4088-80af-6b026ac7a1bb",
   "metadata": {},
   "source": [
    "# Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c7864-57d6-4061-baba-001a97c5ba06",
   "metadata": {},
   "source": [
    "The choice of distance metric in a k-nearest neighbors (KNN) classifier or regressor can significantly impact the performance of the algorithm, as different metrics capture different aspects of similarity or dissimilarity between data points. Two distance metrics Euclidean and Manhattan are commanly used, we discussed the most preffered distance metrics according to the situation below ;\n",
    "\n",
    "**Nature of the data :**\n",
    "- if dataset has continous value - Euclidean distance matix is preffered.\n",
    "- If features are of different scales- Manhattan distance is preffered.\n",
    "\n",
    "**Outliers :**\n",
    "If your dataset contains outliers, Manhattan distance may be more robust due to its use of absolute differences.\n",
    "\n",
    "**Tuning :** \n",
    "By hyperperameter tunnig we can search the best distance matrics for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fae261-f6b0-419b-8d78-edf15d06a85e",
   "metadata": {},
   "source": [
    "# Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893b03b-7f0d-4f88-8ada-38a4d70d61ab",
   "metadata": {},
   "source": [
    "Hyperparameters are the used for incresing the performance of model. Basically hyperparameters making understand tp the model that: what are the requirments of this specific data set. By assigning the best values of the parameters, they works well.Here some of the common parameters of KNN Algorithem are mentioned :\n",
    "\n",
    "  **n_neighbors :** It specifies the number of neighbors to consider when making predictions. A larger value generally smoothens the decision boundary but might lead to over-smoothing.Default value of n_neighbours in Knn is 5.\n",
    "\n",
    "  **matrics :** he metric parameter determines the distance measure used to calculate the distance between data points. Common distance metrics include Euclidean distance ('euclidean'), Manhattan distance ('manhattan').\n",
    "\n",
    "  **algorithm:** The algorithm parameter defines the algorithm used to compute the nearest neighbors. The options typically include 'auto', 'ball_tree', 'kd_tree', and 'brute'. The default is often 'auto', which selects the most appropriate algorithm based on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c3d53-0271-4ec9-be9f-6ff7dc10005a",
   "metadata": {},
   "source": [
    "# Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e76bf-eb31-4848-ac11-0c152d6c53c0",
   "metadata": {},
   "source": [
    "**Impact of Training Set Size:**\n",
    "\n",
    "- small training dataset -------> Overfitting\n",
    "- Large training dataset -------> Best generalizaion but computitonal complexity incresed.\n",
    "\n",
    "**Technique to optimize the size of training dataset :**\n",
    "\n",
    "  **Cross validation :**  use cross-validation techniques, such as k-fold cross-validation, to assess the model's performance with different training set sizes. \n",
    "\n",
    "  **Resampling :** If the dataset is imbalanced, consider resampling techniques such as oversampling the minority class or undersampling the majority class to create a more balanced training set.\n",
    "\n",
    "  **Feature Selection or Dimensionality Reduction:** If the dataset has a large number of features, consider feature selection or dimensionality reduction techniques to reduce the number of features and potentially improve the model's ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f40750-7f50-449e-82b1-9a14bdc677cb",
   "metadata": {},
   "source": [
    "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdac624-2345-42ae-af8e-50101b1890a7",
   "metadata": {},
   "source": [
    "KNN is a very simple algorithem for learners , however some disadvantages of KNN also there. Along with them , we suggest the way to escape these disadvantages even decrese the maximum impact of them ;\n",
    "\n",
    "  **Computational cost :** As Knn alorithem is calculating the distance between all the point to query point for finding the top k- neighbours, it incresed the computational cost of the system.\n",
    "**Solution :** data structures like KD-trees or ball trees are useful in this senario as they reduce the number of distance calculations needed.\n",
    "\n",
    "  **No feature selection :** Another disavantage of KNN is , that it takes all features as equal importance even who are not so co-related with the dependent feature. By giving the equal importance to the unimportant features, KNN performance will decresed.\n",
    "**Solution :** we use PCA to reduce the daimetionsity of the data.\n",
    "\n",
    "  **Outliers :** utliers can have a significant impact on the prediction, especially in lower-dimensional spaces.\n",
    "**Solution :** we can use the distance metrics for reducing the impact of outliers. Manhattan distance, may be less sensitive to outliers compared to the Euclidean distance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
